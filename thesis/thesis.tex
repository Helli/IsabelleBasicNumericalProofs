\documentclass[11pt,a4paper]{article}
\usepackage{isabelle,isabellesym}

% further packages required for unusual symbols (see also
% isabellesym.sty), use only when needed

%\usepackage{amssymb}
  %for \<leadsto>, \<box>, \<diamond>, \<sqsupset>, \<mho>, \<Join>,
  %\<lhd>, \<lesssim>, \<greatersim>, \<lessapprox>, \<greaterapprox>,
  %\<triangleq>, \<yen>, \<lozenge>

%\usepackage{eurosym}
  %for \<euro>

%\usepackage[only,bigsqcap]{stmaryrd}
  %for \<Sqinter>

%\usepackage{eufrak}
  %for \<AA> ... \<ZZ>, \<aa> ... \<zz> (also included in amssymb)

%\usepackage{textcomp}
  %for \<onequarter>, \<onehalf>, \<threequarters>, \<degree>, \<cent>,
  %\<currency>

% this should be the last package used
\usepackage{pdfsetup}

% urls in roman style, theory text in math-similar italics
\urlstyle{rm}
\isabellestyle{it}

% for uniform font size
%\renewcommand{\isastyle}{\isastyleminor}

\newcommand{\snip}[4]
{\expandafter\newcommand\csname #1\endcsname{#4}}
\input{snippets}

\begin{document}

\title{Bachelorarbeit}
\author{Fabian Hellauer}
\maketitle

\tableofcontents

% sane default for proof documents
\parindent 0pt\parskip 0.5ex

\section{Abstract}

Many problems in geometry or topology systems can only be approximated by large computations with long sequences of operations. To best benefit from the speedup of machine computation, one has to make use of the hardwired floating point instructions. However, using the IEEE-floats directly would often create the need for a complicated numerical analysis due to them being affected by round-off.
Another one of their properties removes this need however: Alongside the result, the round-off error of an addition or subtraction can be computed. Storing and using it in further operations thus makes the computation error-free.
We give a simple data format in Isabelle/HOL that uses this approach to provide fast algorithms for error-free addition, subtraction and multiplication.

\section{Introduction}

When attacking computational problems by machine, often times fixed size numbers have to be used: Full implementations of computable numbers are very slow, mostly because they don't use the hardwired floating point operations that are available to modern systems. If the finite set of these machine numbers suffice (or an approximation within their range), the use of hard-wired operations can speed up the computation by a large factor. However, they introduce the problem of round-off, which, when not handled, will affect the output's precision in complex ways.

\subsection{Round-off}
Round-off occurs when a the result of a floating point operation cannot be represented using the fixed size of exponent and mantissa defined by the floating point format. It also occurs when casting measured or infinitely precise data in such a limited precision format.

\paragraph{Dealing with round-off}
If round-off affected arithmetic is used in a long sequence of operations, the result will only approximate within a certain range. Correctness proofs for this range will require a tedious numerical analysis of the algorithms which is very complex to do formally.

\paragraph{Avoiding round-off}
Another approach is to avoid round-off altogether: However, using an implementation of infinitely precise rationals might severely slow down the code's execution due to them not making use of the hardwired floating point operations that modern machines provide.

\paragraph{Float expansions}
If a finite set of numbers with magnitude and precision in the range of IEEE floats suffices however, both the precision and fast execution speed can be preserved: This thesis presents the "float expansion" approach where the accumulated errors are stored in a list alongside with an approximation for the result of the executed sequence. It provides addition, subtraction and multiplication within the numbers representable in this way (a finite superset to IEEE floats).
Only a short operation sequence first described by ... is needed.

\subsection{Contributions}
We provide the data format \typmpf
It follows "float expansion" approach where the accumulated errors are stored in a list alongside with an approximation for the result of the executed sequence. It provides addition, subtraction and multiplication within the numbers representable in this way (a finite superset to IEEE floats).

\subsection{Formalization in Isabelle}
We use the formal setting of Isabelle/HOL to specify and proof correct the procedures, but we make sure all of them can be executed by slightly expanding Isabelle's SML code generation for IEEE-floats.


\subsection{Problem statement}

The task for this bachelor thesis is to provide a multiple precision float arithmetic in Isabelle/HOL. This is described in several scientific papers as "floating point expansion " or "multiple term" strategy. It is an easy way to gain considerable amounts of precision while still using the IEEE floating point specification to enable the widely available acceleration of hardwired operations.

\subsection{Unsortiertes Einleitungszeug}

When attacking computational problems by machine, a trade-off between complexity and speed has to be made: Using an implementation of infinitely precise rationals might severely slow down the code's execution, whereas replacing the sequence of operations by their IEEE745-counterparts will cause round-off errors to appear. Numerical code using hardware floating point operations has to consider the problem of round-off. To avoid the tedious analysis of these accumulating errors

\section{Code Analysis}

The mpf data format is designed to implement the ideas described in [Literaturanalyse] in a very simple way. It is defined as follows:
\mpfdef
where the tuple of a float and a float list should be seen together as a non-empty float list, ordered by decreasing magnitude. The mpf's represented value is the infintite precise sum of all its components. Note that multiple mpfs may represent the same value:
% lemmaexample
% let a = mpf1 b = mpf1 in Val_mpf a = Val_mpf b /\ a != b
Many of these are invalid if we enforce Shewchuck's "non-overlapping" property on the float list, see section ... . We further decrease the allowed representations by not allowing zero components in the list's tail: Since they don't contribute to the mpf's value, omitting them is an easy way to save storage.
The most significant component of the unevaluated sum of floats has a special place in the first tuple component. The list data structure enables the easy removal of zero components in the operations, while the separate handling of the first component makes the simple definition
\newline
\approxdef
\newline
possible. This summand with the special name \constapprox eliminates the need for a test to see if the list is non-empty when reading out the mpf's value. For a discussion as to how good this approximates the lists error-free value, see section     . When considering proof complexity, the special handling of the first component should not impose big problems compared to a standard list, only the induction step (where an element is inserted at position 2) will turn out to look more complicated (see section proofs).

\subsection{IEEE in Isabelle}

The "IEEE standard for floating point arithmetic" (link via bibtex) (IEEE 754-2008) is already modeled in the AFP-theory IEEE\_Floating\_Point/\IEEE.
While the formalization is quite general to accommodate for the many different allowed formats that arise when different values for precision and exponent range are used (the decimal formats are omitted), a strong precedence for the "binary64" format and the "roundTiesToEven" rounding mode can be observed. The functions using this mode are wrapped in definitions with simpler names, e.g. Finite
In the case of the format, this is justifiable by it being in wide and hardware-implemented on most systems.
In the case of the rounding mode, the IEEE standard defines this one to be the standard / default (p. 16).
We will also use this format and rounding mode, which also enables us to use the codeprinting defined in theory CodeFloat. The "TiesToEvenRule" is explicitly proven to be a right choice for the TwoSum-properties[HFPA 4.3 Theorem 4] and thus for all the presented algorithms.

\subsection{Strategy}

The general idea of the presented algorithms is to provide a way of performing error-free computations (within a finite subset of the rational numbers), while still making use of the accelerated hardware operations of IEEE-floats (affected by rounding after each step). This is done by keeping a "record" of all the errors made and storing them in a list alongside the approximation. Through clever cancellation of accumulated errors and removing the resulting zero-components, this list's length can be kept at a moderate level.
A key component in being able to record the errors made upto a point is the TwoSum method first described by (first autor), possibly used earlier. A Central realisation is that for IEEE floating point numbers representing finites, as long as the result of a plus (or minus) operation does not lie outside the representable interval, the error of this calculation can be represented and even computed using again only float-addition and subtraction.

\subsection{Generality}

Only the two defining properties (lemma best-approx and val-preserve) of the TwoSum-method are needed for the mpfs properties of error-free computation. Any software or hardware format that enables such a possibility to add two values and "record" the precise error is in principle suited for error-free computations using these recursive algorithms. Additional lemmas about the format like exact rounding will be needed to make use of Shewchuk's "nonoverlapping" property that he proves to be preserverved by them in (bibtex). Since this property is needed to make assertions about the first components approximation quality (or the maximum expansion length as discussed in sect...), these cannot be provided as HOL-facts yet.

\section{Code Generation}
To profit from the accelerated execution of hardwired float operations, the HOL-code needs to be translated into a compilable language. To this end, Isabelle provides the
%export_code
command. It uses the 
% code printing
statements of the current context. However, due to such conversions being prone to introducing errors, only safe translations are being used by default, i.e. those that preserve the HOL-statements with high certainty. Thus, to enable the generation for hardware float using code, some additional translations need to be added. These should be tested thoroughly to ensure the resulting ML code's correctness.

% Stil im Titel
\subsection{Testing SML's "real" type}

In SML the IEEE-floats are called
% ML_real
and use hardware operations by default. Thus, the translation
% snippethier:
% code_printing type_constructor float
%  (SML) "real" and (OCaml) "float"

from
%theory Code_Float (bibtex)
 
immediately suggests itself. When executing the in section     presented methods however, it turned out that the output was obviously wrong. Tracing this problem back to individual code blocks lead to the conclusion that already the
% constTwoSum
method delivered wrong results: The sum of the two input values did not match the sum of the two output values. Luckily, the error was so large that it was not overcast by the inexact representation as rounded sequence in base 10.
% BeispielIsa2015
This problem with the translated code turned out to stem from an unexpected computation of intermediate results in the "double extended" precision (bibtex IEEE section 3.7). The ML-code used it on some systems and thus computed the error value e as the error how it would be for an addition in extended precision. As this result needed to be translated into the 64-bit format for further usage, it needed to be rounded again at the end of the method. The error of this conversion is not accommodated for in the other output value designed to deliver the correctly rounded result of the 64-addition, thus nullifying the property of error-free transformation. The 80-bit registers are used until storage in a 64-bit value is enforced. Our results are thus not determined by the sequence of operations specified in the code, but by hardly controllable circumstances e.g. the way PolyML handles function calls (Fußnote mit Zitat aus David Matthews' erster Antwort). Worse still, due to different instruction sets being used, the correct behavior was depending on the operating system used (Fußnote mit Zitat aus David Matthews' erster Antwort). One way to circumvent these arbitrary changes in the results is to enforce the storage in a 64-bit value after every floating point operation. From this, the approach of the
% "STORE"-method:
was derived: The result of every addition or subtraction is written to a ML variable (which uses double precision). This value is then used for next operation. This leads to the correct results, but is very slow in execution as shown in section runtime.
As another way of avoiding the unpredictable additional precision, the processors could be set to a mode that does not use this unwanted feature. This could be confirmed through compiling the exported TwoSum code with another ML compiler: running mlton with the flag "-codegen amd64" apparently produces a correct executable for TwoSum. It is desirable to make a similar modification to PolyML. This will also make the
% value
command use the correct code. Kindly, polyml maintainer David Mathews provided a changed version (link zu github) that should work on most systems to run Isabelle with and agreed to consider the behaviour of floats in future polyml-releases. It works by setting the precision to 64-bit in the control word of every float instruction. The change made it to polyML 5.6 (link zu "I committed a change...") and is thus available in Isabelle2016(link zu release notes).

\subsection {Testing the generated SML code}
While the issues described in the section before were fixed, other problems in ML's (or another target language's) float handling might still occur. In addition to the computation being possibly incorrect, two more problems hinder the testing possibilities:
\begin{itemize}
\item The "value [code]" runs into an error because the translation from the computed \MLreal back into a HOL term is not implemented.
\item Evaluating it via the ML command only gives the inexact representation as a rounded sequence in base 10.
\end{itemize}
To make matters worse, the simplifier can't simulate the float operations in HOL due a lack of lemmas for the very abstract definitions of
%IEEE.closest
via an all-quantifier over the \HOLreal type. Thus, we have no way to compute the correct result in our verified setting for a comparison with the SML output.

% TODO: Stil im Titel
\paragraph {Usage of Float.float}

To work around the missing possibility for equality queries on IEEE.floats in HOL, we need to transform the floats in another Format to test the implementation with "Store" against the implementaion without it. To this end, we provide the
%toFloatdefinition
where
%tomanexp
is the
%codemodule
%tomanexpdef
provided by Fabian Immler. It is undefined in HOL. Using this, we can transform IEEE floats to infinitely precise Float.floats and compare the resulting values via their built in
%constequalfloat
function. Implicitely, we use the fact that by design, the finite IEEE-floats are a subset of the of the Float.floats. A disadvantage is that we once again rely on the correctness of a SML function. However, the advantage is that we don't have to provide conversion functions for all of Real.real which would be quite complicated. The sign-mantissa-exponent representation, that both float formats share, makes the transformation back just as easy:
% offloatdef
As desired, a value for mantissa or exponent that does not fit in the SML double float will produce an error during evualtion.

\section{Use cases for mpf}
The MPf data type's precision is only limited by the exponent minimum. It is also absolute in some sense: The value of the least significant bit
%ulpmpf
does not depend on the MPF's magnitude as it would for IEEE-floats alone.
Thus, we define
%ulpmpfdef
While we don't need this definition to make statement about the operations
% italic{within}
the \typmpf type (as they are exact), this is the bound we can achieve on correct conversions from or to infinite precision types.
Since our algorithms are designed to keep a good approximation for
% Valmpf
in the first component, it makes sense to say that the mpf overflows if its first component overflows during one of these operations. This limits the number range. Together with the maximum precision as explained above, we get a finite set of possible mpfs values.
The opposite of an overflow is staying finite: Extending the
%Finite
predicate from
% Code_float, we define
%Finitempfdef
to test if none of the components are NaN or Infinity values. To test for zero components in the error list, we use
%validdef
which is a stronger property as
%lemma validfinite
shows.


As floats, the first and last component are bound in magnitude and precision. Thus, the highest relative precision can be achieved for numbers that are close to overflow in the first component. It corresponds to a 
% \[e_max - e_min + fracwidth = 2*2047 + 52 = 4146] bit binary sequence.
%Grafik number line: HBFA float (p. 18) vs. mpf (gleichverteilt): Any two adjacent MPFs have a fixed difference equal to ulp_float
This diagram also demonstrates the limits of mpf as a whole: Representable are 2 hoch 4146 uniformy distributed numbers between minus threshold floatformat and threshold floatformat. This is enough for many computations, but a clear difference to infinite precision types like
%Realreal
.


\subsection{Input}
The set of numbers representable by mpf is finite.
Input needs to be rounded to one of these expressible values. we provide the
%from_real
function that uses
%Realreal
's operations to iteratively add components corresponing to the current error to the mpf until one such component is zero. This could however also terminate due to the remainer being closer to zero than any non-zero float (the resulting mpf would be rounded), which is why we also provide the
%safe_from_real
function that is supposed to throw an error if this is the case (recognized by the remainder not being zero).
% Die Definitionen als Boxen zwischen dem Text?
Notice that
% "Isvalid a ==> from_real (Val_mpf a) = a"
is not true because the representation might change,
however
%"Isvalid a ==> Val (from_real (Val_mpf a)) = Val_mpf a"
is.

\subsection{Possible Applications}
Only problems where the input is exactly representable as IEEE floats (or measured within their precision) truly benefit from the error-free
%mpf
operations. In other applications, some loss of precision is impossible to avoid. Depending on some numerical properties, these errors might amplify during the course of further computation and make the result inexact again. A main drawback is also that no division operator is available.


\section{Literature review}

All of the consulted authors (Joldes et al., Shewchuck, Priest, HBFA) aim at speeding-up the error-free computation of numbers
%Blatt 2 ... ToDo
focused on using the functional style to make an understandable implemenation where modifications like the zero-component removal or additional restraints on mpf properties (see section .. ) are easily added.
This general setting is much more suited for Isabelle which mainly serves the goal of making assertions about code correctness, not the (very system dependant) execution speed.


% optional bibliography
%\bibliographystyle{abbrv}
%\bibliography{root}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
